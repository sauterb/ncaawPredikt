The particular LSTM used in this project is a part of a sequential Keras model. The model has an input layer, an LSTM layer, two dense layers, a dropout layer, and an output layer. Because the model was not trained on a large batch of data that occurred over a variety of conditions (e.g. time periods and/or teams), overfitting was a significant concern. An overfit model would perform poorly on testing data and, even when correct, be overconfident in its certainty predictions. This is the reason for the dropout layer (rate=0.25). The activations used to introduce nonlinearity into the model were 'ReLU' for the dense layers, 'Sigmoid' for the activation layers (in order to predict a 0-1 probability), and the default ('tanh') for the LSTM layer.  
  
The model is compiled using the 'adam' optimizer. The loss function is binary cross entropy because this is a binary classification problem (WIN=1/LOSS=0). The model is trained on a batch size of 256 over 100 epochs. Further epochs resulted in harmful overfitting. Further results can be seen in the *Technical Results* section. The Keras model summary can be seen in the figure below. 

As touched on in the *Data Processing* section. The features used in the input sequences to the LSTM are previous game's result, current average margin of victory, current RPI ranking (a popular ranking that combines several scheduling and performance metrics), opponent's current RPI ranking, and home-court advantage. 